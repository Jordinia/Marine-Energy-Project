{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d42a19f",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38f367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a131c56",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e0e3bec-f781-4b63-86e3-77e8f860304a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: synthetic_data/weather_data_1.csv\n",
      "File saved: synthetic_data/weather_data_2.csv\n",
      "File saved: synthetic_data/weather_data_3.csv\n",
      "File saved: synthetic_data/weather_data_4.csv\n"
     ]
    }
   ],
   "source": [
    "#Generate Data weather four locations\n",
    "def generate_weather_data(start_date, num_days, num_variables):\n",
    "    # Generate dates\n",
    "    dates = [start_date + timedelta(days=i) for i in range(num_days)]\n",
    "    \n",
    "    # Generate random data for weather variables\n",
    "    data = {f\"Variable_{i+1}\": np.round(np.random.uniform(0, 100, num_days), 2) for i in range(num_variables)}\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    df.insert(0, \"Date\", dates)\n",
    "    return df\n",
    "\n",
    "# Parameters\n",
    "start_date = datetime(2016, 1, 1)\n",
    "num_days = 1460\n",
    "num_variables = 10\n",
    "num_files = 4\n",
    "input_steps = 45\n",
    "output_steps = 14\n",
    "output_dir = \"synthetic_data\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate and save CSV files\n",
    "for i in range(1, num_files + 1):\n",
    "    weather_data = generate_weather_data(start_date, num_days, num_variables)\n",
    "    file_name = os.path.join(output_dir, f\"weather_data_{i}.csv\")\n",
    "    weather_data.to_csv(file_name, index=False)\n",
    "    print(f\"File saved: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9952b6ee-f39d-4cef-af78-0a38b26dc9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Generate data for engine data\n",
    "# num_samples = 1000\n",
    "# output_dir = \"synthetic_data\"  # Reusing the same directory\n",
    "# file_name = \"engine_data.csv\"\n",
    "\n",
    "# # Ensure output directory exists\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Generate random engine data\n",
    "# resistance = np.random.rand(num_samples, 1).astype(np.float32)\n",
    "# power = np.random.rand(num_samples, 1).astype(np.float32)\n",
    "# torque = np.random.rand(num_samples, 1).astype(np.float32)\n",
    "# total_time = np.random.rand(num_samples, 1).astype(np.float32)\n",
    "# distance = np.random.rand(num_samples, 1).astype(np.float32)\n",
    "# total_energy = np.random.rand(num_samples, 1).astype(np.float32)\n",
    "\n",
    "# # Combine into a single DataFrame\n",
    "# engine_data = pd.DataFrame({\n",
    "#     \"resistance\": resistance.flatten(),\n",
    "#     \"power\": power.flatten(),\n",
    "#     \"torque\": torque.flatten(),\n",
    "#     \"total_time\": total_time.flatten(),\n",
    "#     \"distance\": distance.flatten(),\n",
    "#     \"total_energy\": total_energy.flatten()\n",
    "# })\n",
    "\n",
    "# # Save as a single CSV file\n",
    "# file_path = os.path.join(output_dir, file_name)\n",
    "# engine_data.to_csv(file_path, index=False)\n",
    "# print(f\"File saved: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35ec4b0",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0f0312-5f27-4ff2-8e7b-881d640a9a0c",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413eee77-2bfa-4b12-aa47-96e01cc24af8",
   "metadata": {},
   "source": [
    "def load_weather_data_as_timeseries(data_dir, num_variables=10):\n",
    "    time_series_list = []\n",
    "    for i in range(1, 5):  # Assuming 4 weather CSV files\n",
    "        file_path = os.path.join(data_dir, f\"weather_data_{i}.csv\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        time_series_list.append(df.iloc[:, 1:].to_numpy())  # Drop Date column, keep variables\n",
    "    return time_series_list\n",
    "\n",
    "# Load engine data\n",
    "def load_engine_data(data_dir, file):\n",
    "    engine_file_path = os.path.join(data_dir, file)\n",
    "    engine_data = pd.read_csv(engine_file_path)\n",
    "    return engine_data\n",
    "\n",
    "# Min-Max scaling for each time series\n",
    "def scale_time_series(time_series_list):\n",
    "    scalers = []\n",
    "    scaled_series_list = []\n",
    "    for series in time_series_list:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_series = scaler.fit_transform(series)\n",
    "        scalers.append(scaler)\n",
    "        scaled_series_list.append(scaled_series)\n",
    "    return scaled_series_list, scalers\n",
    "\n",
    "# Transform a single time series into input-output samples\n",
    "def transform_time_series_to_samples(series, num_samples=1000, input_steps=45, output_steps=14):\n",
    "    total_steps = input_steps + output_steps\n",
    "    num_rows = series.shape[0] - total_steps + 1\n",
    "    indices = np.random.choice(num_rows, num_samples, replace=False)\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    for idx in indices:\n",
    "        X.append(series[idx : idx + input_steps])\n",
    "        y.append(series[idx + input_steps : idx + total_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Split data for training and testing\n",
    "def split_data(X_list, y_list, engine_data, test_size=0.2, random_state=42):\n",
    "    # Split each weather time series\n",
    "    X_train_list, X_test_list, y_train_list, y_test_list = [], [], [], []\n",
    "    for X, y in zip(X_list, y_list):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "        X_train_list.append(X_train)\n",
    "        X_test_list.append(X_test)\n",
    "        y_train_list.append(y_train)\n",
    "        y_test_list.append(y_test)\n",
    "\n",
    "    # Split engine data\n",
    "    engine_train, engine_test = train_test_split(engine_data, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return X_train_list, X_test_list, y_train_list, y_test_list, engine_train, engine_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fb8b41e-4cd2-472b-9a7b-0dc160a7d82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weather time series: [(1460, 10), (1460, 10), (1460, 10), (1460, 10)]\n",
      "Engine data loaded: (200, 6)\n",
      "Scaled weather time series.\n",
      "torch.Size([800, 45, 4, 10])\n",
      "Engine train shape: torch.Size([160, 1]), Engine test shape: torch.Size([40])\n"
     ]
    }
   ],
   "source": [
    "# Main script\n",
    "data_dir = \"synthetic_data\"\n",
    "dummy_engine_data = \"dummy_engine_data.csv\"\n",
    "\n",
    "# Load weather data as separate time series\n",
    "weather_time_series = load_weather_data_as_timeseries(data_dir)\n",
    "print(\"Loaded weather time series:\", [ts.shape for ts in weather_time_series])\n",
    "\n",
    "# Load engine data\n",
    "engine_data = load_engine_data(data_dir, dummy_engine_data)\n",
    "print(\"Engine data loaded:\", engine_data.shape)\n",
    "\n",
    "# Scale each weather time series\n",
    "scaled_weather_data, weather_scalers = scale_time_series(weather_time_series)\n",
    "print(\"Scaled weather time series.\")\n",
    "\n",
    "# Transform each weather time series into input-output samples\n",
    "weather_X_list, weather_y_list = [], []\n",
    "for series in scaled_weather_data:\n",
    "    X, y = transform_time_series_to_samples(series, num_samples=1000, input_steps=45, output_steps=14)\n",
    "    weather_X_list.append(X)\n",
    "    weather_y_list.append(y)\n",
    "\n",
    "# Combine engine variables into a single array for splitting\n",
    "engine_data_combined = engine_data.to_numpy()\n",
    "\n",
    "# Split the data\n",
    "X_train_list, X_test_list, y_train_list, y_test_list, engine_train, engine_test = split_data(\n",
    "    weather_X_list, weather_y_list, engine_data_combined, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "resistance_train, power_train, torque_train, total_time_train, distance_train, total_energy_train = (\n",
    "    engine_train[:, 0], engine_train[:, 1], engine_train[:, 2], engine_train[:, 3], engine_train[:, 4], engine_train[:, 5]\n",
    ")\n",
    "\n",
    "resistance_test, power_test, torque_test, total_time_test, distance_test, total_energy_test = (\n",
    "    engine_test[:, 0], engine_test[:, 1], engine_test[:, 2], engine_test[:, 3], engine_test[:, 4], engine_test[:, 5]\n",
    ")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensors = [torch.tensor(X, dtype=torch.float32) for X in X_train_list]\n",
    "X_test_tensors = [torch.tensor(X, dtype=torch.float32) for X in X_test_list]\n",
    "\n",
    "resistance_train_tensor = torch.tensor(resistance_train, dtype=torch.float32)\n",
    "power_train_tensor = torch.tensor(power_train, dtype=torch.float32)\n",
    "torque_train_tensor = torch.tensor(torque_train, dtype=torch.float32)\n",
    "total_time_train_tensor = torch.tensor(total_time_train, dtype=torch.float32)\n",
    "distance_train_tensor = torch.tensor(distance_train, dtype=torch.float32)\n",
    "total_energy_train_tensor = torch.tensor(total_energy_train, dtype=torch.float32)\n",
    "\n",
    "resistance_train_tensor=resistance_train_tensor.unsqueeze(1)\n",
    "power_train_tensor=power_train_tensor.unsqueeze(1)\n",
    "torque_train_tensor=torque_train_tensor.unsqueeze(1)\n",
    "total_time_train_tensor=total_time_train_tensor.unsqueeze(1)\n",
    "distance_train_tensor=distance_train_tensor.unsqueeze(1)\n",
    "total_energy_train_tensor = total_energy_train_tensor.unsqueeze(1)\n",
    "\n",
    "resistance_test_tensor = torch.tensor(resistance_test, dtype=torch.float32)\n",
    "power_test_tensor = torch.tensor(power_test, dtype=torch.float32)\n",
    "torque_test_tensor = torch.tensor(torque_test, dtype=torch.float32)\n",
    "total_time_test_tensor = torch.tensor(total_time_test, dtype=torch.float32)\n",
    "distance_test_tensor = torch.tensor(distance_test, dtype=torch.float32)\n",
    "total_energy_test_tensor = torch.tensor(total_energy_test, dtype=torch.float32)\n",
    "\n",
    "X_train_tensors= torch.stack(X_train_tensors, dim=2)\n",
    "X_test_tensors = torch.stack(X_test_tensors, dim=2)\n",
    "print(X_train_tensors.shape)\n",
    "print(f\"Engine train shape: {total_energy_train_tensor.shape}, Engine test shape: {total_energy_test_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2adda5-de18-472e-b84a-b665dbd121c2",
   "metadata": {},
   "source": [
    "# MODEL DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e524020-ee43-4802-a176-0e86de116d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForecastModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ForecastModel, self).__init__()\n",
    "        self.encoder_gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.decoder_gru = nn.GRU(output_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, forecast_steps):\n",
    "        batch_size, time_steps, num_locations, num_features = x.size()\n",
    "        \n",
    "        # Initialize an empty tensor for storing forecasts across locations\n",
    "        forecasts = []\n",
    "\n",
    "        for location in range(num_locations):\n",
    "            # Process each location independently\n",
    "            location_input = x[:, :, location, :]  # Shape: (batch_size, time_steps, num_features)\n",
    "            \n",
    "            # Encoder\n",
    "            _, hidden = self.encoder_gru(location_input)\n",
    "            \n",
    "            # Decoder\n",
    "            decoder_input = torch.zeros(batch_size, 1, num_features).to(x.device)\n",
    "            outputs = []\n",
    "            \n",
    "            for _ in range(forecast_steps):\n",
    "                out, hidden = self.decoder_gru(decoder_input, hidden)\n",
    "                out = self.fc(out.squeeze(1))  # Shape: (batch_size, output_size)\n",
    "                outputs.append(out)\n",
    "                decoder_input = out.unsqueeze(1)\n",
    "            \n",
    "            # Concatenate outputs for each forecast step\n",
    "            location_forecast = torch.stack(outputs, dim=1)  # Shape: (batch_size, forecast_steps, num_features)\n",
    "            forecasts.append(location_forecast)\n",
    "\n",
    "        # Stack forecasts for all locations\n",
    "        forecasts = torch.stack(forecasts, dim=2)  # Shape: (batch_size, forecast_steps, num_locations, num_features)\n",
    "        return forecasts\n",
    "\n",
    "# Define the combined DNN model for total energy prediction\n",
    "class EnergyPredictionModel(nn.Module):\n",
    "    def __init__(self, forecast_output_size, static_input_size, hidden_size):\n",
    "        super(EnergyPredictionModel, self).__init__()\n",
    "        self.forecast_model = ForecastModel(input_size=num_variables, hidden_size=hidden_size, output_size=forecast_output_size)\n",
    "        \n",
    "        # Feed-forward network for total energy prediction\n",
    "        self.fc1 = nn.Linear(forecast_output_size * input_steps * num_files + static_input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)  # Predict a single total energy value\n",
    "    \n",
    "    def forward(self, x_seq, resistance, power, torque, time_total, distance, forecast_steps):\n",
    "        # Forecast model to get the output sequence\n",
    "        forecast_out = self.forecast_model(x_seq, forecast_steps)  # Shape: (batch_size, forecast_steps, num_locations, num_features)\n",
    "        forecast_out_flat = forecast_out.view(forecast_out.size(0), -1)  # Flatten the forecast output across locations and time steps\n",
    "        # Repeat static inputs to match flattened forecast dimensions\n",
    "        static_inputs = torch.cat((resistance, power, torque, time_total, distance), dim=1)  # Shape: (batch_size, 3)\n",
    "        print(forecast_out_flat.shape)\n",
    "        print(static_inputs.shape)\n",
    "        static_inputs=static_inputs.squeeze(1)\n",
    "        print(static_inputs.shape)\n",
    "        # Concatenate static inputs with the flattened forecast output\n",
    "        combined_input = torch.cat((forecast_out_flat, static_inputs), dim=1)\n",
    "        \n",
    "        # Feed-forward layers for total energy prediction\n",
    "        x = torch.relu(self.fc1(combined_input))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        energy_pred = self.fc3(x)  # Output: (batch_size, 1)\n",
    "        \n",
    "        return energy_pred\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = EnergyPredictionModel(forecast_output_size=num_variables, static_input_size=5, hidden_size=64)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8641b3-bfcd-4974-a454-521769bb0346",
   "metadata": {},
   "source": [
    "TRAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cd8efb3-d1ad-41a8-bacf-e6718edb99ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# num_epochs = 10\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Forward pass\n",
    "#     # print(type(X_train_tensors))\n",
    "#     # print(X_train_tensors.shape)\n",
    "#     energy_pred = model(X_train_tensors, resistance_train_tensor, power_train_tensor, torque_train_tensor, total_time_train_tensor, distance_train_tensor, input_steps)\n",
    "#     loss = criterion(energy_pred, total_energy_train_tensor)  # Adjusted to match prediction dimensions\n",
    "    \n",
    "#     # Backward and optimize\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f401d2a4",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f37fcf54-76bc-47ad-a7fc-2dce009015b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "Epoch [1/10], Loss: 0.4080\n",
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "Epoch [2/10], Loss: 0.3536\n",
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "Epoch [3/10], Loss: 0.3104\n",
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "Epoch [4/10], Loss: 0.2680\n",
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "Epoch [5/10], Loss: 0.2270\n",
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "Epoch [6/10], Loss: 0.1879\n",
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "Epoch [7/10], Loss: 0.1528\n",
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "Epoch [8/10], Loss: 0.1205\n",
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "Epoch [9/10], Loss: 0.0956\n",
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "torch.Size([1000, 1])\n",
      "Epoch [10/10], Loss: 0.0823\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Parameters\n",
    "num_features = 10       # Number of input features in the sequence\n",
    "time_steps = 45         # Time steps in the input sequence\n",
    "forecast_steps = 10     # Number of forecast steps (output sequence length)\n",
    "num_locations = 4       # Number of locations\n",
    "hidden_size = 64        # GRU hidden size\n",
    "num_samples = 1000      # Number of data samples\n",
    "\n",
    "# Generate random data\n",
    "np.random.seed(42)\n",
    "X_sequence = np.random.rand(num_samples, time_steps, num_locations, num_features).astype(np.float32)\n",
    "resistance = np.random.rand(num_samples, 1).astype(np.float32)  # Single value per sample\n",
    "power = np.random.rand(num_samples, 1).astype(np.float32)       # Single value per sample\n",
    "torque = np.random.rand(num_samples, 1).astype(np.float32)      # Single value per sample\n",
    "total_energy = np.random.rand(num_samples, 1).astype(np.float32)  # Single value per sample\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_sequence = torch.tensor(X_sequence)\n",
    "resistance = torch.tensor(resistance)\n",
    "power = torch.tensor(power)\n",
    "torque = torch.tensor(torque)\n",
    "total_energy = torch.tensor(total_energy)\n",
    "\n",
    "# Define the Seq2Seq Forecast model with GRU encoder and decoder\n",
    "class ForecastModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ForecastModel, self).__init__()\n",
    "        self.encoder_gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.decoder_gru = nn.GRU(output_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, forecast_steps):\n",
    "        batch_size, time_steps, num_locations, num_features = x.size()\n",
    "        \n",
    "        # Initialize an empty tensor for storing forecasts across locations\n",
    "        forecasts = []\n",
    "\n",
    "        for location in range(num_locations):\n",
    "            # Process each location independently\n",
    "            location_input = x[:, :, location, :]  # Shape: (batch_size, time_steps, num_features)\n",
    "            \n",
    "            # Encoder\n",
    "            _, hidden = self.encoder_gru(location_input)\n",
    "            \n",
    "            # Decoder\n",
    "            decoder_input = torch.zeros(batch_size, 1, num_features).to(x.device)\n",
    "            outputs = []\n",
    "            \n",
    "            for _ in range(forecast_steps):\n",
    "                out, hidden = self.decoder_gru(decoder_input, hidden)\n",
    "                out = self.fc(out.squeeze(1))  # Shape: (batch_size, output_size)\n",
    "                outputs.append(out)\n",
    "                decoder_input = out.unsqueeze(1)\n",
    "            \n",
    "            # Concatenate outputs for each forecast step\n",
    "            location_forecast = torch.stack(outputs, dim=1)  # Shape: (batch_size, forecast_steps, num_features)\n",
    "            forecasts.append(location_forecast)\n",
    "\n",
    "        # Stack forecasts for all locations\n",
    "        forecasts = torch.stack(forecasts, dim=2)  # Shape: (batch_size, forecast_steps, num_locations, num_features)\n",
    "        return forecasts\n",
    "\n",
    "# Define the combined DNN model for total energy prediction\n",
    "class EnergyPredictionModel(nn.Module):\n",
    "    def __init__(self, forecast_output_size, static_input_size, hidden_size):\n",
    "        super(EnergyPredictionModel, self).__init__()\n",
    "        self.forecast_model = ForecastModel(input_size=num_features, hidden_size=hidden_size, output_size=forecast_output_size)\n",
    "        \n",
    "        # Feed-forward network for total energy prediction\n",
    "        self.fc1 = nn.Linear(forecast_output_size * forecast_steps * num_locations + static_input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)  # Predict a single total energy value\n",
    "    \n",
    "    def forward(self, x_seq, resistance, power, torque, forecast_steps):\n",
    "        # Forecast model to get the output sequence\n",
    "        forecast_out = self.forecast_model(x_seq, forecast_steps)  # Shape: (batch_size, forecast_steps, num_locations, num_features)\n",
    "        forecast_out_flat = forecast_out.view(forecast_out.size(0), -1)  # Flatten the forecast output across locations and time steps\n",
    "        print(resistance.shape)\n",
    "        print(power.shape)\n",
    "        print(torque.shape)\n",
    "        # Repeat static inputs to match flattened forecast dimensions\n",
    "        static_inputs = torch.cat((resistance, power, torque), dim=1)  # Shape: (batch_size, 3)\n",
    "        \n",
    "        # Concatenate static inputs with the flattened forecast output\n",
    "        combined_input = torch.cat((forecast_out_flat, static_inputs), dim=1)\n",
    "        \n",
    "        # Feed-forward layers for total energy prediction\n",
    "        x = torch.relu(self.fc1(combined_input))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        energy_pred = self.fc3(x)  # Output: (batch_size, 1)\n",
    "        \n",
    "        return energy_pred\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = EnergyPredictionModel(forecast_output_size=num_features, static_input_size=3, hidden_size=hidden_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    energy_pred = model(X_sequence, resistance, power, torque, forecast_steps)\n",
    "    loss = criterion(energy_pred, total_energy)  # Adjusted to match prediction dimensions\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde5f0fc",
   "metadata": {},
   "source": [
    "## Output Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24708274",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
